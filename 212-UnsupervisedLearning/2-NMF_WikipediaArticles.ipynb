{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative matrix factorization (NMF) for Documents\n",
    "\n",
    "Dimention reduction technique\n",
    "\n",
    "models are interpretable and easy to explain (unline PCA)\n",
    "\n",
    "all samples MUST be non-negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess `wikipedia-vectors.csv` into the format in which we'll used it in the exercises, \n",
    "# we have to take its transpose:\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "filePath = \"../datasets/Wikipedia articles/wikipedia-vectors.csv\"\n",
    "df = pd.read_csv(filePath, index_col=0)\n",
    "articles = csr_matrix(df.transpose())\n",
    "titles = list(df.columns)\n",
    "\n",
    "# The reason for taking this transpose is that without it, there would be 13,000 columns \n",
    "# (corresponding to the 13,000 words in the file), which is a lot of columns for a CSV to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HTTP 404</th>\n",
       "      <td>0.035435</td>\n",
       "      <td>0.039641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alexa Internet</th>\n",
       "      <td>0.064925</td>\n",
       "      <td>0.052661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Internet Explorer</th>\n",
       "      <td>0.050883</td>\n",
       "      <td>0.038062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HTTP cookie</th>\n",
       "      <td>0.033883</td>\n",
       "      <td>0.037170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google Search</th>\n",
       "      <td>0.037928</td>\n",
       "      <td>0.034032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1\n",
       "HTTP 404           0.035435  0.039641\n",
       "Alexa Internet     0.064925  0.052661\n",
       "Internet Explorer  0.050883  0.038062\n",
       "HTTP cookie        0.033883  0.037170\n",
       "Google Search      0.037928  0.034032"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 13125)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF applied to Wikipedia articles\n",
    "Applying NMF, using the tf-idf word-frequency array of Wikipedia articles, given as a csr matrix articles. Here, fit the model and transform the articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.44]\n",
      " [0.   0.   0.   0.   0.   0.56]\n",
      " [0.   0.   0.   0.   0.   0.4 ]\n",
      " [0.   0.   0.   0.   0.   0.38]\n",
      " [0.   0.   0.   0.   0.   0.48]\n",
      " [0.01 0.01 0.01 0.03 0.   0.33]\n",
      " [0.   0.   0.02 0.   0.01 0.36]\n",
      " [0.   0.   0.   0.   0.   0.49]\n",
      " [0.02 0.01 0.   0.02 0.03 0.48]\n",
      " [0.01 0.03 0.03 0.07 0.02 0.34]\n",
      " [0.   0.   0.53 0.   0.03 0.  ]\n",
      " [0.   0.   0.35 0.   0.   0.  ]\n",
      " [0.01 0.01 0.31 0.06 0.01 0.02]\n",
      " [0.   0.01 0.34 0.01 0.   0.  ]\n",
      " [0.   0.   0.43 0.   0.04 0.  ]\n",
      " [0.   0.   0.48 0.   0.   0.  ]\n",
      " [0.01 0.02 0.37 0.03 0.   0.01]\n",
      " [0.   0.   0.48 0.   0.   0.  ]\n",
      " [0.   0.01 0.55 0.   0.   0.  ]\n",
      " [0.   0.   0.46 0.   0.   0.  ]\n",
      " [0.   0.01 0.02 0.51 0.06 0.01]\n",
      " [0.   0.   0.   0.51 0.   0.  ]\n",
      " [0.   0.01 0.   0.42 0.   0.  ]\n",
      " [0.   0.   0.   0.43 0.   0.  ]\n",
      " [0.   0.   0.   0.49 0.   0.  ]\n",
      " [0.1  0.09 0.   0.38 0.   0.01]\n",
      " [0.   0.   0.   0.57 0.   0.01]\n",
      " [0.01 0.01 0.   0.47 0.   0.01]\n",
      " [0.   0.   0.   0.57 0.   0.  ]\n",
      " [0.   0.   0.   0.52 0.01 0.01]\n",
      " [0.   0.41 0.   0.   0.   0.  ]\n",
      " [0.   0.6  0.   0.01 0.   0.  ]\n",
      " [0.01 0.26 0.   0.02 0.01 0.  ]\n",
      " [0.   0.64 0.   0.   0.   0.  ]\n",
      " [0.   0.61 0.   0.   0.   0.  ]\n",
      " [0.   0.34 0.   0.   0.   0.  ]\n",
      " [0.01 0.31 0.02 0.   0.01 0.  ]\n",
      " [0.01 0.21 0.01 0.05 0.02 0.01]\n",
      " [0.01 0.46 0.   0.02 0.   0.  ]\n",
      " [0.   0.64 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.47 0.  ]\n",
      " [0.   0.   0.   0.   0.49 0.  ]\n",
      " [0.   0.   0.   0.   0.38 0.01]\n",
      " [0.   0.   0.   0.01 0.54 0.  ]\n",
      " [0.   0.   0.01 0.   0.42 0.  ]\n",
      " [0.   0.   0.   0.   0.51 0.  ]\n",
      " [0.   0.   0.   0.   0.37 0.  ]\n",
      " [0.   0.   0.04 0.   0.23 0.  ]\n",
      " [0.01 0.   0.02 0.01 0.32 0.04]\n",
      " [0.   0.   0.   0.   0.42 0.  ]\n",
      " [0.3  0.   0.   0.   0.   0.  ]\n",
      " [0.36 0.   0.   0.   0.   0.  ]\n",
      " [0.39 0.03 0.   0.02 0.   0.02]\n",
      " [0.37 0.   0.   0.04 0.   0.01]\n",
      " [0.43 0.   0.   0.   0.   0.  ]\n",
      " [0.45 0.   0.   0.   0.   0.  ]\n",
      " [0.27 0.   0.   0.05 0.   0.02]\n",
      " [0.44 0.   0.   0.   0.01 0.  ]\n",
      " [0.29 0.01 0.01 0.01 0.19 0.01]\n",
      " [0.37 0.01 0.   0.1  0.01 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)\n",
    "# Print the NMF features\n",
    "print(nmf_features.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF features of the Wikipedia articles\n",
    "Now we will explore the NMF features we created above. \n",
    "\n",
    "When investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. Why? NMF components represent topics (for instance, acting!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.003815\n",
      "1    0.000000\n",
      "2    0.000000\n",
      "3    0.571887\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Anne Hathaway, dtype: float64\n",
      "0    0.000000\n",
      "1    0.005575\n",
      "2    0.000000\n",
      "3    0.419579\n",
      "4    0.000000\n",
      "5    0.000000\n",
      "Name: Denzel Washington, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a pandas DataFrame: df\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway',])\n",
    "# Print the row for 'Denzel Washington'\n",
    "print(df.loc['Denzel Washington',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the array: 13125\n",
      "First 10 words: ['aaron', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abbas', 'abbey', 'abbreviated', 'abbreviation', 'abc']\n"
     ]
    }
   ],
   "source": [
    "def read_txt_to_array(filepath):\n",
    "    \"\"\"\n",
    "    Reads a text file and returns its contents as an array of strings,\n",
    "    where each element is a line from the file.\n",
    "\n",
    "    Args:\n",
    "        filepath: The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, where each string is a line from the file.\n",
    "        Returns an empty list if the file does not exist or if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            lines = file.read().splitlines()\n",
    "        return lines\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage with the provided file path:\n",
    "filepath = \"../datasets/Wikipedia articles/wikipedia-vocabulary-utf8.txt\"\n",
    "words = read_txt_to_array(filepath)\n",
    "\n",
    "# Print the array (optional - for demonstration)\n",
    "# print(word_array)\n",
    "\n",
    "# Print the number of words in the array\n",
    "print(f\"Number of words in the array: {len(words)}\")\n",
    "\n",
    "# Print the first 10 words in the array (optional - for demonstration)\n",
    "print(f\"First 10 words: {words[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF learns topics of documents\n",
    "When NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. \n",
    "\n",
    "Previously, we saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. Now we are identifying the topic of the corresponding NMF component.\n",
    "\n",
    "Here, we can recognize the topic that the articles about Anne Hathaway and Denzel Washington have in common!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 13125)\n",
      "film       0.632082\n",
      "award      0.254825\n",
      "starred    0.246927\n",
      "role       0.212867\n",
      "actress    0.187646\n",
      "Name: 3, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame: components_df\n",
    "components_df = pd.DataFrame(model.components_, columns=words)\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(components_df.shape)\n",
    "\n",
    "# Select row 3: component\n",
    "component = components_df.iloc[3,]\n",
    "\n",
    "# Print result of nlargest\n",
    "print(component.nlargest())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
